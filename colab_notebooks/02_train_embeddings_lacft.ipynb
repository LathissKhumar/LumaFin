{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LumaFin - Label-Aware Contrastive Fine-Tuning (L-A CFT)\n",
    "\n",
    "This notebook fine-tunes the sentence transformer embedding model using contrastive learning.\n",
    "\n",
    "**What this notebook does:**\n",
    "1. Loads training data from Google Drive\n",
    "2. Creates contrastive triplets (anchor, positive, negative)\n",
    "3. Fine-tunes sentence-transformers model with contrastive loss\n",
    "4. Saves the fine-tuned model to Google Drive\n",
    "\n",
    "**Runtime:** GPU Required (T4 or better recommended)\n",
    "**Time:** ~30-60 minutes for full training\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT:** Make sure to enable GPU runtime:  \n",
    "Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Mount Google Drive and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import torch\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è WARNING: No GPU detected. Training will be VERY slow.\")\n",
    "    print(\"   Please enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sentence-transformers transformers torch pandas numpy scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load from Google Drive\n",
    "train_file = '/content/drive/MyDrive/LumaFin/data/train.csv'\n",
    "\n",
    "if not os.path.exists(train_file):\n",
    "    print(\"‚ùå Training data not found. Please run notebook 01 first.\")\n",
    "else:\n",
    "    df_train = pd.read_csv(train_file)\n",
    "    print(f\"‚úÖ Loaded {len(df_train)} training examples\")\n",
    "    print(f\"\\nCategory distribution:\")\n",
    "    print(df_train['category'].value_counts())\n",
    "    print(f\"\\nSample:\")\n",
    "    print(df_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Contrastive Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "# Group examples by category\n",
    "category_to_examples = defaultdict(list)\n",
    "for _, row in df_train.iterrows():\n",
    "    text = f\"{row['merchant']} {row.get('description', '')} ${row['amount']:.2f}\"\n",
    "    category_to_examples[row['category']].append(text)\n",
    "\n",
    "categories = list(category_to_examples.keys())\n",
    "print(f\"‚úÖ Grouped into {len(categories)} categories\")\n",
    "for cat in categories:\n",
    "    print(f\"  {cat}: {len(category_to_examples[cat])} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_triplets(category_to_examples, num_triplets_per_example=3):\n",
    "    \"\"\"Create anchor-positive-negative triplets for contrastive learning.\"\"\"\n",
    "    triplets = []\n",
    "    \n",
    "    for category, examples in category_to_examples.items():\n",
    "        if len(examples) < 2:\n",
    "            continue  # Skip categories with too few examples\n",
    "        \n",
    "        for anchor in examples:\n",
    "            for _ in range(num_triplets_per_example):\n",
    "                # Positive: same category, different example\n",
    "                positive_candidates = [e for e in examples if e != anchor]\n",
    "                if not positive_candidates:\n",
    "                    continue\n",
    "                positive = random.choice(positive_candidates)\n",
    "                \n",
    "                # Negative: different category\n",
    "                negative_category = random.choice([c for c in category_to_examples.keys() if c != category])\n",
    "                negative = random.choice(category_to_examples[negative_category])\n",
    "                \n",
    "                triplets.append((anchor, positive, negative))\n",
    "    \n",
    "    return triplets\n",
    "\n",
    "print(\"Creating triplets...\")\n",
    "triplets = create_triplets(category_to_examples, num_triplets_per_example=2)\n",
    "print(f\"‚úÖ Created {len(triplets)} triplets\")\n",
    "\n",
    "# Show sample\n",
    "if triplets:\n",
    "    anchor, positive, negative = triplets[0]\n",
    "    print(f\"\\nSample triplet:\")\n",
    "    print(f\"  Anchor:   {anchor}\")\n",
    "    print(f\"  Positive: {positive}\")\n",
    "    print(f\"  Negative: {negative}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Load Base Model and Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load base model\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "print(f\"Loading model: {model_name}\")\n",
    "model = SentenceTransformer(model_name)\n",
    "print(f\"‚úÖ Model loaded. Embedding dimension: {model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert triplets to InputExamples\n",
    "print(\"Converting to InputExamples...\")\n",
    "train_examples = []\n",
    "for anchor, positive, negative in triplets:\n",
    "    train_examples.append(InputExample(texts=[anchor, positive, negative]))\n",
    "\n",
    "print(f\"‚úÖ Created {len(train_examples)} training examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "batch_size = 16  # Adjust based on GPU memory\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "print(f\"‚úÖ DataLoader created with batch size {batch_size}\")\n",
    "print(f\"   Total batches: {len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "num_epochs = 3  # Increase for better results (3-5 epochs recommended)\n",
    "warmup_steps = int(len(train_dataloader) * num_epochs * 0.1)  # 10% warmup\n",
    "\n",
    "# Use TripletLoss for contrastive learning\n",
    "train_loss = losses.TripletLoss(model=model)\n",
    "\n",
    "print(f\"‚úÖ Training configuration:\")\n",
    "print(f\"   Epochs: {num_epochs}\")\n",
    "print(f\"   Batch size: {batch_size}\")\n",
    "print(f\"   Warmup steps: {warmup_steps}\")\n",
    "print(f\"   Total training steps: {len(train_dataloader) * num_epochs}\")\n",
    "print(f\"   Loss function: TripletLoss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Output path\n",
    "output_path = '/content/drive/MyDrive/LumaFin/models/lumafin-lacft-v1.0'\n",
    "\n",
    "print(f\"üöÄ Starting training...\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Train\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=num_epochs,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path=output_path,\n",
    "    show_progress_bar=True,\n",
    "    save_best_model=True,\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\n‚úÖ Training complete! Time: {training_time/60:.1f} minutes\")\n",
    "print(f\"‚úÖ Model saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model\n",
    "finetuned_model = SentenceTransformer(output_path)\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"Starbucks coffee $5.50\",\n",
    "    \"Uber ride $15.00\",\n",
    "    \"Netflix subscription $15.99\",\n",
    "    \"Walmart groceries $45.30\",\n",
    "    \"Doctor visit $120.00\"\n",
    "]\n",
    "\n",
    "print(\"Testing embeddings on sample queries:\\n\")\n",
    "for query in test_queries:\n",
    "    embedding = finetuned_model.encode(query)\n",
    "    print(f\"‚úÖ {query}\")\n",
    "    print(f\"   Embedding shape: {embedding.shape}, Norm: {np.linalg.norm(embedding):.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Compare Base vs Fine-Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load base model\n",
    "base_model = SentenceTransformer(model_name)\n",
    "\n",
    "# Test similarity\n",
    "query1 = \"Starbucks coffee\"\n",
    "query2 = \"Dunkin donuts\"  # Same category\n",
    "query3 = \"Uber ride\"  # Different category\n",
    "\n",
    "# Base model\n",
    "base_emb1 = base_model.encode([query1])\n",
    "base_emb2 = base_model.encode([query2])\n",
    "base_emb3 = base_model.encode([query3])\n",
    "\n",
    "# Fine-tuned model\n",
    "ft_emb1 = finetuned_model.encode([query1])\n",
    "ft_emb2 = finetuned_model.encode([query2])\n",
    "ft_emb3 = finetuned_model.encode([query3])\n",
    "\n",
    "print(\"Similarity Comparison:\\n\")\n",
    "print(f\"Query 1: {query1}\")\n",
    "print(f\"Query 2: {query2} (same category - should be HIGH)\")\n",
    "print(f\"Query 3: {query3} (different category - should be LOW)\\n\")\n",
    "\n",
    "print(\"BASE MODEL:\")\n",
    "print(f\"  Similarity(Q1, Q2): {cosine_similarity(base_emb1, base_emb2)[0][0]:.3f}\")\n",
    "print(f\"  Similarity(Q1, Q3): {cosine_similarity(base_emb1, base_emb3)[0][0]:.3f}\\n\")\n",
    "\n",
    "print(\"FINE-TUNED MODEL:\")\n",
    "print(f\"  Similarity(Q1, Q2): {cosine_similarity(ft_emb1, ft_emb2)[0][0]:.3f}\")\n",
    "print(f\"  Similarity(Q1, Q3): {cosine_similarity(ft_emb1, ft_emb3)[0][0]:.3f}\\n\")\n",
    "\n",
    "print(\"‚úÖ Fine-tuned model should show higher similarity for same-category pairs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Training Complete!\n",
    "\n",
    "Your fine-tuned embedding model is ready and saved to:\n",
    "```\n",
    "/content/drive/MyDrive/LumaFin/models/lumafin-lacft-v1.0\n",
    "```\n",
    "\n",
    "### Next Steps:\n",
    "1. **Run notebook 03_train_reranker.ipynb** to train the XGBoost reranker\n",
    "2. **Run notebook 04_evaluate_pipeline.ipynb** to test the complete system\n",
    "\n",
    "### To use this model in your local repository:\n",
    "1. Download the model folder from Google Drive\n",
    "2. Place it in `models/embeddings/lumafin-lacft-v1.0`\n",
    "3. Update `.env` file:\n",
    "   ```\n",
    "   MODEL_PATH=models/embeddings/lumafin-lacft-v1.0\n",
    "   ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

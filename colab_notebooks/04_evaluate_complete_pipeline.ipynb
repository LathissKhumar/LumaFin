{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LumaFin - Complete Pipeline Evaluation\n",
    "\n",
    "This notebook evaluates the complete LumaFin categorization pipeline.\n",
    "\n",
    "**What this notebook does:**\n",
    "1. Loads all trained models (embeddings, FAISS, reranker)\n",
    "2. Runs comprehensive evaluation on test set\n",
    "3. Compares baseline vs fine-tuned performance\n",
    "4. Generates detailed accuracy reports\n",
    "5. Creates visualizations of results\n",
    "\n",
    "**Runtime:** GPU helpful for embeddings, but CPU OK\n",
    "**Time:** ~10-20 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sentence-transformers xgboost faiss-cpu pandas numpy scikit-learn matplotlib seaborn tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Models and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "\n",
    "# Load test data\n",
    "test_file = '/content/drive/MyDrive/LumaFin/data/test.csv'\n",
    "df_test = pd.read_csv(test_file)\n",
    "print(f\"âœ… Test set: {len(df_test)} examples\")\n",
    "print(f\"\\nCategory distribution:\")\n",
    "print(df_test['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding models\n",
    "base_model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "finetuned_path = '/content/drive/MyDrive/LumaFin/models/lumafin-lacft-v1.0'\n",
    "\n",
    "print(\"Loading embedding models...\")\n",
    "base_model = SentenceTransformer(base_model_name)\n",
    "print(f\"âœ… Base model loaded\")\n",
    "\n",
    "if os.path.exists(finetuned_path):\n",
    "    finetuned_model = SentenceTransformer(finetuned_path)\n",
    "    print(f\"âœ… Fine-tuned model loaded\")\n",
    "else:\n",
    "    print(f\"âš ï¸ Fine-tuned model not found, will use base model only\")\n",
    "    finetuned_model = base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FAISS index and metadata\n",
    "faiss_path = '/content/drive/MyDrive/LumaFin/models/faiss_index.bin'\n",
    "metadata_path = '/content/drive/MyDrive/LumaFin/models/faiss_metadata.pkl'\n",
    "\n",
    "if os.path.exists(faiss_path) and os.path.exists(metadata_path):\n",
    "    index = faiss.read_index(faiss_path)\n",
    "    with open(metadata_path, 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    \n",
    "    train_categories = metadata['categories']\n",
    "    all_categories = metadata['all_categories']\n",
    "    \n",
    "    print(f\"âœ… FAISS index loaded: {index.ntotal} vectors\")\n",
    "    print(f\"âœ… Categories: {all_categories}\")\n",
    "else:\n",
    "    print(\"âŒ FAISS index not found. Please run notebook 03 first.\")\n",
    "    raise FileNotFoundError(\"FAISS index required\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reranker model\n",
    "reranker_path = '/content/drive/MyDrive/LumaFin/models/xgb_reranker.pkl'\n",
    "\n",
    "if os.path.exists(reranker_path):\n",
    "    with open(reranker_path, 'rb') as f:\n",
    "        reranker_model = pickle.load(f)\n",
    "    print(f\"âœ… Reranker model loaded\")\n",
    "else:\n",
    "    print(\"âš ï¸ Reranker not found. Will evaluate retrieval only.\")\n",
    "    reranker_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_text(row):\n",
    "    desc = row.get('description', '')\n",
    "    return f\"{row['merchant']} {desc} ${row['amount']:.2f}\"\n",
    "\n",
    "def retrieve_top_k(query_text, model, index, k=20):\n",
    "    \"\"\"Retrieve top-k candidates using FAISS.\"\"\"\n",
    "    query_emb = model.encode([query_text])[0].astype('float32')\n",
    "    query_emb = query_emb / np.linalg.norm(query_emb)\n",
    "    \n",
    "    scores, indices = index.search(np.array([query_emb]), k)\n",
    "    candidates = [(train_categories[i], scores[0][j]) for j, i in enumerate(indices[0])]\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "def predict_by_voting(candidates):\n",
    "    \"\"\"Predict category by majority voting.\"\"\"\n",
    "    votes = Counter([cat for cat, _ in candidates])\n",
    "    return votes.most_common(1)[0][0]\n",
    "\n",
    "def predict_by_reranker(candidates, all_categories, reranker_model):\n",
    "    \"\"\"Predict category using reranker.\"\"\"\n",
    "    # Aggregate scores by category\n",
    "    category_scores = {cat: [] for cat in all_categories}\n",
    "    for cat, score in candidates:\n",
    "        category_scores[cat].append(score)\n",
    "    \n",
    "    # Extract features\n",
    "    features = []\n",
    "    for cat in all_categories:\n",
    "        scores = category_scores[cat]\n",
    "        feat = [\n",
    "            len(scores),\n",
    "            sum(scores) if scores else 0,\n",
    "            max(scores) if scores else 0,\n",
    "            np.mean(scores) if scores else 0,\n",
    "            min(scores) if scores else 0,\n",
    "            len(scores) / len(candidates) if candidates else 0,\n",
    "            0\n",
    "        ]\n",
    "        features.append(feat)\n",
    "    \n",
    "    X = np.array(features)\n",
    "    probs = reranker_model.predict_proba(X)[:, 1]\n",
    "    pred_idx = np.argmax(probs)\n",
    "    \n",
    "    return all_categories[pred_idx], probs[pred_idx]\n",
    "\n",
    "print(\"âœ… Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate Retrieval (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "print(\"Evaluating retrieval baseline (base model + voting)...\\n\")\n",
    "\n",
    "predictions_baseline = []\n",
    "true_labels = []\n",
    "\n",
    "for _, row in tqdm(df_test.iterrows(), total=len(df_test), desc=\"Baseline\"):\n",
    "    query_text = create_text(row)\n",
    "    candidates = retrieve_top_k(query_text, base_model, index, k=20)\n",
    "    pred = predict_by_voting(candidates)\n",
    "    \n",
    "    predictions_baseline.append(pred)\n",
    "    true_labels.append(row['category'])\n",
    "\n",
    "accuracy_baseline = accuracy_score(true_labels, predictions_baseline)\n",
    "print(f\"\\nâœ… Baseline Accuracy: {accuracy_baseline:.1%}\")\n",
    "print(f\"\\nðŸ“Š Classification Report:\")\n",
    "print(classification_report(true_labels, predictions_baseline, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating fine-tuned model + voting...\\n\")\n",
    "\n",
    "predictions_finetuned = []\n",
    "\n",
    "for _, row in tqdm(df_test.iterrows(), total=len(df_test), desc=\"Fine-tuned\"):\n",
    "    query_text = create_text(row)\n",
    "    candidates = retrieve_top_k(query_text, finetuned_model, index, k=20)\n",
    "    pred = predict_by_voting(candidates)\n",
    "    \n",
    "    predictions_finetuned.append(pred)\n",
    "\n",
    "accuracy_finetuned = accuracy_score(true_labels, predictions_finetuned)\n",
    "print(f\"\\nâœ… Fine-tuned Accuracy: {accuracy_finetuned:.1%}\")\n",
    "print(f\"\\nðŸ“Š Classification Report:\")\n",
    "print(classification_report(true_labels, predictions_finetuned, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate with Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if reranker_model:\n",
    "    print(\"Evaluating fine-tuned model + reranker...\\n\")\n",
    "    \n",
    "    predictions_reranker = []\n",
    "    confidences_reranker = []\n",
    "    \n",
    "    for _, row in tqdm(df_test.iterrows(), total=len(df_test), desc=\"Reranker\"):\n",
    "        query_text = create_text(row)\n",
    "        candidates = retrieve_top_k(query_text, finetuned_model, index, k=20)\n",
    "        pred, conf = predict_by_reranker(candidates, all_categories, reranker_model)\n",
    "        \n",
    "        predictions_reranker.append(pred)\n",
    "        confidences_reranker.append(conf)\n",
    "    \n",
    "    accuracy_reranker = accuracy_score(true_labels, predictions_reranker)\n",
    "    print(f\"\\nâœ… Reranker Accuracy: {accuracy_reranker:.1%}\")\n",
    "    print(f\"   Average Confidence: {np.mean(confidences_reranker):.3f}\")\n",
    "    print(f\"\\nðŸ“Š Classification Report:\")\n",
    "    print(classification_report(true_labels, predictions_reranker, zero_division=0))\n",
    "else:\n",
    "    print(\"âš ï¸ Skipping reranker evaluation (model not loaded)\")\n",
    "    predictions_reranker = None\n",
    "    accuracy_reranker = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Comparison and Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Accuracy comparison\n",
    "results = {\n",
    "    'Baseline\\n(Base + Voting)': accuracy_baseline,\n",
    "    'Fine-tuned\\n(L-A CFT + Voting)': accuracy_finetuned,\n",
    "}\n",
    "\n",
    "if accuracy_reranker is not None:\n",
    "    results['Complete\\n(Fine-tuned + Reranker)'] = accuracy_reranker\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(results.keys(), [v * 100 for v in results.values()], \n",
    "               color=['#3498db', '#2ecc71', '#e74c3c'])\n",
    "plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "plt.title('LumaFin Pipeline Performance Comparison', fontsize=14, fontweight='bold')\n",
    "plt.ylim(0, 100)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.1f}%',\n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/drive/MyDrive/LumaFin/results_comparison.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Comparison chart saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for best model\n",
    "if predictions_reranker:\n",
    "    best_predictions = predictions_reranker\n",
    "    model_name = \"Complete Pipeline\"\n",
    "else:\n",
    "    best_predictions = predictions_finetuned\n",
    "    model_name = \"Fine-tuned Model\"\n",
    "\n",
    "cm = confusion_matrix(true_labels, best_predictions, labels=all_categories)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=all_categories, yticklabels=all_categories)\n",
    "plt.title(f'Confusion Matrix - {model_name}', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Category', fontsize=12)\n",
    "plt.xlabel('Predicted Category', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/drive/MyDrive/LumaFin/confusion_matrix.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Confusion matrix saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Per-Category Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Calculate per-category metrics\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    true_labels, best_predictions, labels=all_categories, zero_division=0\n",
    ")\n",
    "\n",
    "# Create DataFrame\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Category': all_categories,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1,\n",
    "    'Support': support\n",
    "})\n",
    "\n",
    "metrics_df = metrics_df.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(\"\\nðŸ“Š Per-Category Performance:\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "metrics_df.to_csv('/content/drive/MyDrive/LumaFin/per_category_metrics.csv', index=False)\n",
    "print(\"\\nâœ… Metrics saved to per_category_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize per-category F1 scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(metrics_df['Category'], metrics_df['F1-Score'], color='#3498db')\n",
    "plt.xlabel('Category', fontsize=12)\n",
    "plt.ylabel('F1 Score', fontsize=12)\n",
    "plt.title('Per-Category F1 Scores', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add horizontal line for average\n",
    "avg_f1 = metrics_df['F1-Score'].mean()\n",
    "plt.axhline(y=avg_f1, color='r', linestyle='--', label=f'Average: {avg_f1:.3f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/drive/MyDrive/LumaFin/per_category_f1.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Per-category chart saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Generate Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report\n",
    "report = f\"\"\"\n",
    "# LumaFin Evaluation Report\n",
    "\n",
    "## Dataset\n",
    "- Test Examples: {len(df_test)}\n",
    "- Categories: {len(all_categories)}\n",
    "- Training Index Size: {index.ntotal} vectors\n",
    "\n",
    "## Performance Summary\n",
    "\n",
    "### Model Accuracy\n",
    "| Model | Accuracy |\n",
    "|-------|----------|\n",
    "| Baseline (Base + Voting) | {accuracy_baseline:.1%} |\n",
    "| Fine-tuned (L-A CFT + Voting) | {accuracy_finetuned:.1%} |\n",
    "\"\"\"\n",
    "\n",
    "if accuracy_reranker:\n",
    "    report += f\"| Complete (Fine-tuned + Reranker) | {accuracy_reranker:.1%} |\\n\"\n",
    "\n",
    "report += f\"\"\"\n",
    "\n",
    "### Improvement\n",
    "- Fine-tuning gain: {(accuracy_finetuned - accuracy_baseline)*100:+.1f}%\n",
    "\"\"\"\n",
    "\n",
    "if accuracy_reranker:\n",
    "    report += f\"- Reranker gain: {(accuracy_reranker - accuracy_finetuned)*100:+.1f}%\\n\"\n",
    "    report += f\"- Total improvement: {(accuracy_reranker - accuracy_baseline)*100:+.1f}%\\n\"\n",
    "\n",
    "report += f\"\"\"\n",
    "\n",
    "### Per-Category Performance\n",
    "- Average F1-Score: {metrics_df['F1-Score'].mean():.3f}\n",
    "- Best Category: {metrics_df.iloc[0]['Category']} (F1={metrics_df.iloc[0]['F1-Score']:.3f})\n",
    "- Worst Category: {metrics_df.iloc[-1]['Category']} (F1={metrics_df.iloc[-1]['F1-Score']:.3f})\n",
    "\n",
    "## Files Generated\n",
    "- `results_comparison.png` - Accuracy comparison chart\n",
    "- `confusion_matrix.png` - Confusion matrix heatmap\n",
    "- `per_category_f1.png` - Per-category F1 scores\n",
    "- `per_category_metrics.csv` - Detailed metrics table\n",
    "\n",
    "## Conclusion\n",
    "The LumaFin pipeline achieves **{max(accuracy_baseline, accuracy_finetuned, accuracy_reranker or 0):.1%} accuracy** on the test set.\n",
    "\"\"\"\n",
    "\n",
    "if accuracy_reranker and accuracy_reranker >= 0.90:\n",
    "    report += \"\\nâœ… **TARGET ACHIEVED: >90% accuracy**\"\n",
    "\n",
    "# Save report\n",
    "with open('/content/drive/MyDrive/LumaFin/evaluation_report.md', 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(report)\n",
    "print(\"\\nâœ… Report saved to evaluation_report.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Evaluation Complete!\n",
    "\n",
    "All results have been saved to your Google Drive at:\n",
    "```\n",
    "/content/drive/MyDrive/LumaFin/\n",
    "```\n",
    "\n",
    "### Generated Files:\n",
    "- ðŸ“Š `evaluation_report.md` - Complete evaluation summary\n",
    "- ðŸ“ˆ `results_comparison.png` - Model comparison chart\n",
    "- ðŸŽ¯ `confusion_matrix.png` - Confusion matrix\n",
    "- ðŸ“‰ `per_category_f1.png` - Category performance\n",
    "- ðŸ“‹ `per_category_metrics.csv` - Detailed metrics\n",
    "\n",
    "### Next Steps for Hackathon:\n",
    "1. Download all trained models and results from Google Drive\n",
    "2. Integrate into your local repository\n",
    "3. Prepare demo presentation\n",
    "4. Test the API and Streamlit UI\n",
    "5. Create demo video showcasing the system"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

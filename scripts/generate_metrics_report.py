#!/usr/bin/env python3
"""Generate a consolidated metrics report from evaluation outputs.

This script will:
- Run `scripts/evaluate.py` if requested (mode and limit configurable)
- Or load an existing evaluation JSON (if provided)
- Compute macro / per-class precision, recall, F1 and confusion matrix
- Save a Markdown report to `reports/metrics_report.md` and a PNG confusion matrix to `reports/confusion_matrix.png`

Usage:
  PYTHONPATH=. python scripts/generate_metrics_report.py --run-eval --mode fusion --limit 500
  PYTHONPATH=. python scripts/generate_metrics_report.py --input evaluation_results.json
"""
from __future__ import annotations

import argparse
import json
import os
import subprocess
from collections import Counter
from pathlib import Path
from typing import Dict, List

import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    precision_recall_fscore_support,
)

ROOT = Path(__file__).resolve().parents[1]
REPORT_DIR = ROOT / "reports"
REPORT_DIR.mkdir(exist_ok=True)


def run_evaluation(mode: str, limit: int) -> Path:
    """Run the project's evaluation script and return the path to the JSON results.

    The evaluation script is expected to write a JSON file named like `evaluation_results_*.json`.
    """
    cmd = ["python", "scripts/evaluate.py", "--mode", mode, "--limit", str(limit)]
    print("Running evaluation:", " ".join(cmd))
    subprocess.check_call(cmd, cwd=ROOT)

    # Find the most recent evaluation JSON
    eval_files = sorted(ROOT.glob("evaluation_results*.json"), key=lambda p: p.stat().st_mtime, reverse=True)
    if not eval_files:
        raise FileNotFoundError("No evaluation_results JSON found after running evaluation")
    return eval_files[0]


def load_results(path: Path) -> Dict:
    with open(path, "r") as f:
        return json.load(f)


def compute_metrics(y_true: List[str], y_pred: List[str], labels: List[str]) -> Dict:
    # Per-class PRF
    precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, labels=labels, zero_division=0)
    macro_f1 = float(np.mean(f1))
    report = {
        "labels": labels,
        "precision": precision.tolist(),
        "recall": recall.tolist(),
        "f1": f1.tolist(),
        "support": support.tolist(),
        "macro_f1": macro_f1,
    }
    return report


def plot_confusion(y_true: List[str], y_pred: List[str], labels: List[str], out_path: Path):
    cm = confusion_matrix(y_true, y_pred, labels=labels)
    fig, ax = plt.subplots(figsize=(8, 6))
    im = ax.imshow(cm, interpolation="nearest", cmap=plt.cm.Blues)
    ax.figure.colorbar(im, ax=ax)
    ax.set(
        xticks=np.arange(len(labels)),
        yticks=np.arange(len(labels)),
        xticklabels=labels,
        yticklabels=labels,
        ylabel="True label",
        xlabel="Predicted label",
        title="Confusion Matrix",
    )
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor")

    # Annotate
    thresh = cm.max() / 2.0 if cm.size else 0
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], "d"), ha="center", va="center", color="white" if cm[i, j] > thresh else "black")

    fig.tight_layout()
    fig.savefig(out_path, dpi=200)
    plt.close(fig)


def write_markdown(report_path: Path, metrics: Dict, conf_img_path: Path, extra: Dict = None):
    labels = metrics["labels"]
    with open(report_path, "w") as f:
        f.write("# Metrics Report\n\n")
        f.write("Generated by `scripts/generate_metrics_report.py`\n\n")
        if extra:
            f.write("## Run details\n")
            for k, v in extra.items():
                f.write(f"- **{k}**: {v}\n")
            f.write("\n")
        f.write("## Summary\n\n")
        f.write(f"- **Macro F1:** {metrics['macro_f1']:.4f}\n")
        f.write(f"- **Total samples:** {int(sum(metrics['support']))}\n\n")

        f.write("## Per-class metrics\n\n")
        f.write("| Label | Precision | Recall | F1 | Support |\n")
        f.write("|---|---:|---:|---:|---:|\n")
        for i, lab in enumerate(labels):
            f.write(f"| {lab} | {metrics['precision'][i]:.3f} | {metrics['recall'][i]:.3f} | {metrics['f1'][i]:.3f} | {int(metrics['support'][i])} |\n")
        f.write("\n")

        f.write("## Confusion Matrix\n\n")
        f.write(f"![Confusion Matrix]({conf_img_path.name})\n\n")

        f.write("## Notes & Recommendations\n\n")
        f.write("- Focus on classes with low recall to improve coverage.\n")
        f.write("- Consider more negative sampling during training if many false positives exist for a class.\n")
        f.write("- Calibrate the reranker probabilities (Platt scaling or isotonic) if confidence is poorly calibrated.\n")


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--run-eval", action="store_true", help="Run scripts/evaluate.py before generating report")
    parser.add_argument("--mode", default="fusion", help="Evaluation mode to run (retrieval/ reranker / fusion)")
    parser.add_argument("--limit", type=int, default=500, help="Number of samples to evaluate when running evaluator")
    parser.add_argument("--input", type=str, default=None, help="Path to an existing evaluation JSON to use instead of running evaluation")
    args = parser.parse_args()

    if args.run_eval and args.input:
        print("Cannot both run evaluator and load input; choose one")
        return

    if args.run_eval:
        eval_path = run_evaluation(args.mode, args.limit)
    elif args.input:
        eval_path = Path(args.input)
    else:
        # pick most recent evaluation_results*.json
        candidates = sorted(ROOT.glob("evaluation_results*.json"), key=lambda p: p.stat().st_mtime, reverse=True)
        if not candidates:
            raise FileNotFoundError("No evaluation_results*.json found. Run with --run-eval or provide --input")
        eval_path = candidates[0]

    print("Using evaluation file:", eval_path)
    data = load_results(eval_path)

    # Handle two possible evaluation formats:
    # 1) List of per-sample dicts with true_label/pred_label (detailed)
    # 2) Aggregated summary with `per_class` stats (precision/recall/f1/support)
    y_true = []
    y_pred = []
    labels = []

    if isinstance(data, list):
        for row in data:
            t = row.get("true_label") or row.get("label") or row.get("ground_truth")
            p = row.get("pred_label") or row.get("prediction") or row.get("predicted_label")
            if t is None or p is None:
                t = row.get("y_true") or t
                p = row.get("y_pred") or p
            if t is None or p is None:
                continue
            y_true.append(str(t))
            y_pred.append(str(p))

        labels = sorted(list(set(y_true) | set(y_pred)))
        metrics = compute_metrics(y_true, y_pred, labels)
        conf_img = REPORT_DIR / "confusion_matrix.png"
        plot_confusion(y_true, y_pred, labels, conf_img)
        report_md = REPORT_DIR / "metrics_report.md"
        extra = {"eval_file": str(eval_path), "samples": len(y_true)}
        write_markdown(report_md, metrics, conf_img, extra)
    elif isinstance(data, dict) and "per_class" in data:
        per = data["per_class"]
        labels = sorted(per.keys())
        precision = [per[l].get("precision", 0.0) for l in labels]
        recall = [per[l].get("recall", 0.0) for l in labels]
        f1 = [per[l].get("f1", 0.0) for l in labels]
        support = [per[l].get("support", 0) for l in labels]
        metrics = {
            "labels": labels,
            "precision": precision,
            "recall": recall,
            "f1": f1,
            "support": support,
            "macro_f1": float(data.get("macro", {}).get("f1", float(np.mean(f1)) if f1 else 0.0)),
        }
        # Synthesize an approximate confusion matrix from per-class precision/recall/support.
        # NOTE: This is an approximation when raw per-sample predictions are not available.
        n = len(labels)
        cm = np.zeros((n, n), dtype=int)
        # Compute diagonal (true positives) from recall * support
        for i in range(n):
            tp = int(round(recall[i] * support[i]))
            cm[i, i] = tp
        # Distribute false negatives (support - tp) across other classes.
        for i in range(n):
            fn = support[i] - int(cm[i, i])
            if fn <= 0:
                continue
            # Prefer distributing errors to classes with lower precision (more likely to receive false positives)
            other_idxs = [j for j in range(n) if j != i]
            precisions = np.array([precision[j] for j in other_idxs], dtype=float)
            # If all precisions are 1.0 (or zero), distribute uniformly
            if np.allclose(precisions, 1.0) or np.allclose(precisions, 0.0):
                weights = np.ones(len(other_idxs), dtype=float)
            else:
                # Higher (1 - precision) means more likely to get false positives
                weights = 1.0 - precisions
                # clip negatives
                weights = np.clip(weights, 0.0, None)
                if weights.sum() == 0:
                    weights = np.ones(len(other_idxs), dtype=float)
            props = (weights / weights.sum()) * fn
            props = np.round(props).astype(int)
            # fix rounding mismatch
            delta = fn - props.sum()
            if delta != 0:
                props[0] += int(delta)
            for idx_j, val in zip(other_idxs, props):
                cm[i, idx_j] += int(val)

        # Create an image for the synthetic confusion matrix
        conf_img = REPORT_DIR / "synth_confusion_matrix.png"
        fig, ax = plt.subplots(figsize=(8, 6))
        im = ax.imshow(cm, interpolation="nearest", cmap=plt.cm.Blues)
        ax.figure.colorbar(im, ax=ax)
        ax.set(
            xticks=np.arange(n),
            yticks=np.arange(n),
            xticklabels=labels,
            yticklabels=labels,
            ylabel="True label",
            xlabel="Predicted label",
            title="Approximate Confusion Matrix (synthesized from aggregated stats)",
        )
        plt.setp(ax.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor")
        thresh = cm.max() / 2.0 if cm.size else 0
        for i in range(cm.shape[0]):
            for j in range(cm.shape[1]):
                ax.text(j, i, format(cm[i, j], "d"), ha="center", va="center", color="white" if cm[i, j] > thresh else "black")
        fig.tight_layout()
        fig.savefig(conf_img, dpi=200)
        plt.close(fig)

        report_md = REPORT_DIR / "metrics_report.md"
        extra = {"eval_file": str(eval_path), "samples": int(data.get("support_total", sum(support)))}
        write_markdown(report_md, metrics, conf_img, extra)
    else:
        raise ValueError("Unsupported evaluation JSON format")
    print("Report written:", report_md)


if __name__ == "__main__":
    main()
